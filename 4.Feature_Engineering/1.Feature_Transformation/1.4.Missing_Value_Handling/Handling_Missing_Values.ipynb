{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: cyan; text-align: center; font-size: 250%; font-weight: bold;\">\n",
    "  Handling of Missing Values\n",
    "</h1>\n",
    "\n",
    "<h2 style=\"color: lightblue; text-align: left; font-size: 130%; font-weight: bold;\">\n",
    "  List of the Content:\n",
    "</h2>\n",
    "        <ol style=\"font-size: 120%;\">\n",
    "        <li>Detection of Missing Values/Info of Dataset\n",
    "            <ul style=\"font-size: 90%; margin-top: 5px; color: coral\">\n",
    "            <li>Statistical Summary</li>\n",
    "            <li>Visualisation of Missing values-MissingNo library</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li> Diagnosis of Type of Missing Values</li>\n",
    "              <ul style=\"font-size: 90%; margin-top: 5px;  color: coral\">\n",
    "              <li>By Visual Inspections</li>\n",
    "              <li>By Statistical Tests</li>\n",
    "              </ul>\n",
    "<li>Methods for Handling Missing Values</li>\n",
    "  <ul style=\"font-size: 90%; margin-top: 5px; color: coral\">\n",
    "    <li>Deletion (Pairwise | Listwise | Dropping Entire Columns)</li>\n",
    "    <li>Imputation\n",
    "      <ul>\n",
    "      <li>Univariate\n",
    "          <ul>\n",
    "              <li>In case of Numerical Data</li>\n",
    "              <li>In case of Categorical Data</li>\n",
    "              <li>Special Case</li>\n",
    "          </ul>\n",
    "      </li>\n",
    "    <li>Multivariate</li>\n",
    "        <ul>\n",
    "              <li>KNN | MICE </li>\n",
    "              <li>Special Case</li>\n",
    "        </ul>\n",
    "      </ul>\n",
    "    </li>\n",
    "  </ul>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:gold; text-align:left; font-size:200%; font-weight:bold;\">\n",
    "1) Detection/Info of Missing Values</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:deeppink; text-align:left; font-size:150%; font-weight:bold;\">\n",
    "1.A) Statistical Summary-General Info</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "df=pd.read_csv('heart_disease.csv')\n",
    "df.sample(3)\n",
    "miss_value=df.isnull().sum()\n",
    "miss_value[miss_value>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:magenta; text-align:left; font-size:110%; font-weight:bold;\">\n",
    "Proportion of Missing Values-Detailed Info</h2>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('heart_disease.csv')\n",
    "def missing_values(df, output='summary'):\n",
    "    #1) Total missing values per column\n",
    "    miss_val = df.isnull().sum()\n",
    "    #2) Percentage of missing values per column\n",
    "    percentage_miss_val = (miss_val / len(df)) * 100\n",
    "    #3) Create a DataFrame with the results\n",
    "    miss_val_table = pd.concat([miss_val, percentage_miss_val], axis=1)\n",
    "    miss_val_table.columns= ['Missing Values','% of Total Values']\n",
    "    # 5) Sort the table by percentage of missing values in descending order\n",
    "    sorted_miss_val_table = miss_val_table[miss_val_table.iloc[:, 0] != 0].sort_values('% of Total Values', ascending=False).round(1)\n",
    "    # retun based on the specific value \n",
    "    if output=='miss_value':\n",
    "        return miss_value\n",
    "    elif output=='percentage_miss_val':\n",
    "        return percentage_miss_val\n",
    "    else :\n",
    "        return sorted_miss_val_table\n",
    "# Example usage with your data\n",
    "missing_data_summary = missing_values(data, output='miss_value')\n",
    "miss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:deeppink; text-align:left; font-size:150%; font-weight:bold;\">\n",
    "1.B) Visualisation of Missing values-MissingNo library</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import missingno as msno\n",
    "df=pd.read_csv('heart_disease.csv')\n",
    "#1) Bar chart \n",
    "msno.bar(df)\n",
    "#2) Matrix chart\n",
    "msno.matrix(df)\n",
    "#3) Heat map \n",
    "msno.heatmap(df)\n",
    "#4) dendragram  \n",
    "msno.dendrogram(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:gold; text-align:left; font-size:200%; font-weight:bold;\">\n",
    "2) Diagnosis of Type of Missing Values</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:deeppink; text-align:left; font-size:150%; font-weight:bold;\">\n",
    "2.A) By Visualisation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Load DataFrame\n",
    "air_df = pd.read_csv('air_quality_data.csv')\n",
    "\n",
    "# Identify Missing Values\n",
    "BM_MV = air_df['NO2_Location_A'].isna()  # Boolean Series for Missing Values in NO2_Location_A\n",
    "# Split Data\n",
    "group_1 = air_df.loc[BM_MV, ['Temperature', 'NO2_Location_A']]  # Rows with missing NO2_Location_A\n",
    "group_2 = air_df.loc[~BM_MV, ['Temperature', 'NO2_Location_A']]  # Rows without missing NO2_Location_A\n",
    "\n",
    "# Plot the Data\n",
    "plt.figure(figsize=(10, 5))\n",
    "# Box Plot\n",
    "plt.subplot(131)\n",
    "box_sr = [air_df.loc[BM_MV, 'Temperature'], air_df.loc[~BM_MV, 'Temperature']]\n",
    "MV_labels = ['With Missing Values', 'Without Missing Values']\n",
    "plt.boxplot(box_sr, vert=False)\n",
    "plt.yticks([1, 2], MV_labels)\n",
    "plt.title('Box Plot')\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(132)\n",
    "plt.hist(air_df.loc[BM_MV, 'Temperature'], bins=5, alpha=0.5, label='With Missing Values', color='r')\n",
    "plt.title('Hist-With Missing values')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.hist(air_df.loc[~BM_MV, 'Temperature'], bins=5, alpha=0.5, label='Without Missing Values', color='g')\n",
    "plt.title('Hist-Without Missing values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:deeppink; text-align:left; font-size:150%; font-weight:bold;\">\n",
    "2.B) By Statistical Tests</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load DataFrame\n",
    "air_df = pd.read_csv('air_quality_data.csv')\n",
    "\n",
    "#Identify Missing Values\n",
    "BM_MV = air_df['NO2_Location_A'].isna()  # Boolean Series for Missing Values in NO2_Location_A\n",
    "\n",
    "# Split Data\n",
    "group_a = air_df.loc[BM_MV, 'Temperature']  # Rows with missing NO2_Location_A\n",
    "group_b = air_df.loc[~BM_MV, 'Temperature']  # Rows without missing NO2_Location_A\n",
    "\n",
    "# Perform the two-sample t-test independent \n",
    "t_statistic, p_value = stats.ttest_ind(group_a, group_b, equal_var=False)  # Use equal_var=False for unequal variance\n",
    "\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the Null Hypothesis: Significant difference between the groups\")\n",
    "else:\n",
    "    print(\"Accepting the Null Hypothesis: No significant difference between the groups\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:gold; text-align:left; font-size:200%; font-weight:bold;\">\n",
    "3) Methods for handling missing values  </h2>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:deeppink; text-align:left; font-size:150%; font-weight:bold;\">\n",
    "3.A) Deletions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "df=pd.read_csv('data_science_job.csv')\n",
    "#Infomation about missing values\n",
    "miss_value=df.isnull().sum()\n",
    "per_miss_values=100*miss_value/len(df)\n",
    "miss_val_info_table= pd.concat([miss_value, per_miss_values], axis=1).round(1)\n",
    "miss_val_info_table.columns=['Missing Values','% of Total Values'] \n",
    "miss_val_info_table[miss_val_info_table.iloc[:, 0] !=0]\n",
    "\n",
    "\n",
    "#1) Deletions \n",
    "#------ i want to delete those rows in which % of missing value is <5% \n",
    "cols_no_miss= []\n",
    "for var in df.columns:\n",
    "    percent_missing= df[var].isnull().sum()*100/len(df)\n",
    "    if 0<percent_missing<5:\n",
    "        cols_no_miss.append(var)\n",
    "type(cols_no_miss)\n",
    "#-------------df with cols having <5% missing data\n",
    "df_new=df[cols_no_miss]\n",
    "#-------------dropping the missing value in this cols \n",
    "df_new=df_new.dropna()\n",
    "print(f\"Original shape: {df.shape}, shape after deletions: {df_new.shape}\")\n",
    "\n",
    "#---------------Now checking whetehr Data is MCAR:------------\n",
    "\n",
    "#<<<<<<<---------FOR NUMERICAL VARIABLES----------->>>>>>\n",
    "\n",
    "# A) Ploting the Histogram \n",
    "import matplotlib.pyplot as plt \n",
    "fig=plt.figure(figsize=(8,3))\n",
    "plt.subplot(121)\n",
    "df['training_hours'].hist(bins=50,density=True, color='r' )\n",
    "df_new['training_hours'].hist(bins=50, density=True ,color='g' )\n",
    "#B) Probability graph\n",
    "plt.subplot(122) \n",
    "df['training_hours'].plot.density(color='r' )\n",
    "df_new['training_hours'].plot.density(color='g')\n",
    "plt.show()\n",
    "\n",
    "#<<<<<<<---------FOR CATEGORICAL VARIABLES----------->>>>>>\n",
    "#RATIO of observation per categories  \n",
    "temp=pd.concat([\n",
    "    df['enrolled_university'].value_counts()/len(df), #in original data \n",
    "    df_new['enrolled_university'].value_counts()/len(df)  #in new df dat\n",
    "    \n",
    "], axis=1)\n",
    "temp.columns=['original', 'after deletion']\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:deeppink; text-align:left; font-size:170%; font-weight:bold;\">\n",
    "3.B) Imputations</h2>\n",
    "<h2 style=\"color:deeppink; text-align:left; font-size:170%; font-weight:bold;\">\n",
    "3.B.1.A) Univariate: For Numerical Data </h2>\n",
    "  \n",
    "<h2 style=\"color:magenta; text-align:left; font-size:140%; font-weight:bold;\">\n",
    "3.B.1.A.i) Mean | Median Imputation</h2>\n",
    "\n",
    "<h2 style=\"color:coral; text-align:left; font-size:120%; font-weight:bold;\">\n",
    "Using Pandas Library</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "df=pd.read_csv('titanic_toy.csv')\n",
    "#checing the info % of missiness \n",
    "df.isnull().mean()\n",
    "\n",
    "#TRAIN-TEST SPLIT \n",
    "x=df.iloc[:, :-1]\n",
    "y=df.iloc[:, -1]\n",
    "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.2 , random_state=0)\n",
    "\n",
    "#Transformation -----USING PANDAS----------- as age and fare have null values\n",
    "mean_age=x_train['Age'].mean()\n",
    "median_age=x_train['Age'].median()\n",
    "mean_fare=x_train['Fare'].mean()\n",
    "median_fare=x_train['Fare'].median()\n",
    "# creating X_train and test DF having 4-Imputed cols\n",
    "x_train['age_mean']=x_train['Age'].fillna(mean_age)\n",
    "x_train['age_median']=x_train['Age'].fillna(median_age)\n",
    "x_train['fare_mean']=x_train['Fare'].fillna(mean_fare)\n",
    "x_train['fare_median']=x_train['Fare'].fillna(median_fare)\n",
    "\n",
    "\n",
    "#Checking following after imputations \n",
    "#a) Change in shapep--check change in variance\n",
    "print(\"Original Age var:\", x_train['Age'].var(),\"\\n\"\n",
    "      \"      After Mean-Age Imputation var:\",x_train['age_mean'].var(),\"\\n\"\n",
    "      \"      After Median-Age Imputation var:\",x_train['age_median'].var()\n",
    "     )\n",
    "print(\"Original Fare var:\", x_train['Fare'].var(),\"\\n\"\n",
    "      \"      After Mean-Fare Imputation var:\", x_train['fare_mean'].var(),\"\\n\"\n",
    "      \"      After Median-Fare Imputation var:\", x_train['fare_median'].var()\n",
    "     )\n",
    "#b)plotting to check the distribution \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "x_train['Age'].plot(kind='kde', color='r' )\n",
    "x_train['age_mean'].plot(kind='kde', color='b' )\n",
    "x_train['age_median'].plot(kind='kde', color='g' )\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "x_train['Fare'].plot(kind='kde', color='r' )\n",
    "x_train['fare_mean'].plot(kind='kde', color='b' )\n",
    "x_train['fare_median'].plot(kind='kde', color='g' )\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#c) Covariance \n",
    "x_train.cov()\n",
    "\n",
    "#d) Draw box plot \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "x_train[['Age', 'age_median', 'age_median']].boxplot()\n",
    "plt.subplot(122)\n",
    "x_train[['Fare', 'fare_mean', 'fare_median']].boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:coral; text-align:left; font-size:150%; font-weight:bold;\">\n",
    "Using Scikitlearn Library</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#Data ingestion \n",
    "df=pd.read_csv('titanic_toy.csv')\n",
    "#train-test split \n",
    "x=df.iloc[:, :-1]\n",
    "y=df.iloc[:, -1]\n",
    "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Using simple imputer\n",
    "imputer1=SimpleImputer(strategy='mean')\n",
    "imputer2=SimpleImputer(strategy='median')\n",
    "\n",
    "trf=ColumnTransformer([\n",
    "    ('imputer1',imputer1, ['Age']),\n",
    "    ('imputer2', imputer2, ['Fare'])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "#Fit and transform \n",
    "trf.fit(x_train)\n",
    "x_train_trans=trf.transform(x_train)\n",
    "x_test_trans=trf.transform(x_test)\n",
    "\n",
    "#Now Perform the basic check there is np RED FLAG\n",
    "x_train_trans_df=pd.DataFrame(x_train_trans, columns=x_train.columns)\n",
    "\n",
    "#a) Change in shapep--check change in variance\n",
    "print(\"Original Age var:\", x_train['Age'].var(),\"\\n\"\n",
    "      \"After Mean-Age Imputation var:\",x_train_trans_df['Age'].var()\n",
    "     )\n",
    "print(\"Original Fare var:\", x_train['Fare'].var(),\"\\n\"\n",
    "      \"After Median-Fare Imputation var:\", x_train_trans_df['Fare'].var()\n",
    "      )\n",
    "#b)plotting to check the distribution \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "x_train['Age'].plot(kind='kde', color='r')\n",
    "x_train_trans_df['Age'].plot(kind='kde', color='g' )\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "x_train['Fare'].plot(kind='kde', color='r' )\n",
    "x_train_trans_df['Fare'].plot(kind='kde', color='g' )\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#c) Draw box plot \n",
    "df_box = pd.DataFrame({\n",
    "    'Original Age': x_train['Age'],\n",
    "    'Transformed Age': x_train_trans_df['Age'], \n",
    "    'Original Fare': x_train['Fare'],\n",
    "    'Transformed Fare': x_train_trans_df['Fare']\n",
    "    })\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "df_box[['Original Age', 'Transformed Age']].boxplot()\n",
    "plt.subplot(122)\n",
    "df_box[['Original Fare','Transformed Fare']].boxplot()\n",
    "plt.show()\n",
    "#c) Covariance \n",
    "df_box.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:magenta; text-align:left; font-size:140%; font-weight:bold;\">\n",
    "3.B.1.A.ii) Arbitrary Value Imputations </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'City': ['New York', 'Los Angeles', np.nan, 'Chicago', np.nan], \n",
    "        'income': [100, '99', np.nan, '80', np.nan]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Instantiate the SimpleImputer with strategy 'constant' and fill_value 'Unknown'\n",
    "imputer1 = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "imputer2 = SimpleImputer(strategy='constant', fill_value=-9999)\n",
    "\n",
    "# Fit and transform the data\n",
    "df['City'] = imputer1.fit_transform(df[['City']]).ravel()\n",
    "df['income'] = imputer2.fit_transform(df[['income']]).ravel()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:magenta; text-align:left; font-size:140%; font-weight:bold;\">\n",
    "3.B.1.A.iv) Random Sampling value imputation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = {'Age': [25, np.nan, 35, 45, np.nan, 32],\n",
    "        'Salary': [50000, 54000, np.nan, 62000, np.nan, 58000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate into two subsets\n",
    "df_non_missing = df.dropna()  # Subset with no missing values\n",
    "df_missing = df[df.isnull().any(axis=1)]  # Subset with missing values\n",
    "\n",
    "# Imputation function\n",
    "def random_sample_imputation(df_non_missing, df_missing, column):\n",
    "    \"\"\"\n",
    "    Fills missing values in 'column' of df_missing with random samples from df_non_missing.\n",
    "    \"\"\"\n",
    "    random_sample = df_non_missing[column].sample(df_missing[column].isnull().sum(), replace=True)\n",
    "    random_sample.index = df_missing[df_missing[column].isnull()].index\n",
    "    df_missing.loc[df_missing[column].isnull(), column] = random_sample\n",
    "\n",
    "# Apply random sample imputation\n",
    "for column in df.columns:\n",
    "    if df[column].isnull().sum() > 0:  # If there are missing values in the column\n",
    "        random_sample_imputation(df_non_missing, df, column)\n",
    "\n",
    "print(\"DataFrame after Random Sample Imputation:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:deeppink; text-align:left; font-size:170%; font-weight:bold;\">\n",
    "3.B.1.B) Univariate: For Categorical Data </h2>\n",
    "\n",
    "<h2 style=\"color:magenta; text-align:left; font-size:140%; font-weight:bold;\">\n",
    "3.B.1.B.i) Mode (Most Frequent value) Imputations </h2>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "#Data ingestion \n",
    "df=pd.read_csv('train.csv')\n",
    "df=df[['FireplaceQu', 'GarageQual', 'SalePrice' ]]\n",
    "#train-test split \n",
    "x=df.iloc[:, :-1]\n",
    "y=df.iloc[:, -1]\n",
    "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "#Using simple imputer\n",
    "imputer=SimpleImputer(strategy='most_frequent')\n",
    "#Fit and transform \n",
    "x_train_trans=imputer.fit_transform(x_train)\n",
    "x_test_trans=imputer.transform(x_test)\n",
    "#...................Ater this it can be use any algo--------------------------------------- \n",
    "\n",
    "######################-----------CHECKING Whether mode changes Dist------############\n",
    "\n",
    "#--------------------- a) Kde Plot with Fireplace Qu -------------------------------\n",
    "df_subset_not_null = df[df['FireplaceQu'] == 'Gd']['SalePrice']\n",
    "df_subset_is_null = df[df['FireplaceQu'].isnull()]['SalePrice']\n",
    "plt.figure(figsize=(12, 4)) \n",
    "# First subplot\n",
    "plt.subplot(121)\n",
    "df_subset_not_null.plot(kind='kde', label='FireplaceQu with Gd')\n",
    "df_subset_is_null.plot(kind='kde', label='FireplaceQu with NA')\n",
    "# Now fill the subset_is_null with mode\n",
    "df_trans = imputer.fit_transform(df)\n",
    "df_trans_data = pd.DataFrame(df_trans, columns=df.columns)\n",
    "df_trans_data[df_trans_data['FireplaceQu']=='Gd']['SalePrice'].plot(kind='kde', label='After Imputation')\n",
    "plt.legend()\n",
    "\n",
    "#--------------------- b) Kde Plot with GarageQual -------------------------------\n",
    "df_subset_not_null = df[df['GarageQual'] == 'TA']['SalePrice']\n",
    "df_subset_is_null = df[df['GarageQual'].isnull()]['SalePrice']\n",
    "# Second subplot\n",
    "plt.subplot(122)\n",
    "df_subset_not_null.plot(kind='kde', label='GarageQual with TA')\n",
    "df_subset_is_null.plot(kind='kde', label='GarageQual with NA')\n",
    "# Now fill the subset_is_null with mode\n",
    "df_trans = imputer.fit_transform(df)\n",
    "df_trans_data = pd.DataFrame(df_trans, columns=df.columns)\n",
    "df_trans_data[df_trans_data['GarageQual']=='TA']['SalePrice'].plot(kind='kde', label='After Imputation')\n",
    "plt.legend()\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:deeppink; text-align:left; font-size:170%; font-weight:bold;\">\n",
    "3.B.1.C) Univariate:Special Case </h2>\n",
    "\n",
    "<h2 style=\"color:magenta; text-align:left; font-size:140%; font-weight:bold;\">\n",
    "3.B.1.C.i) Missing Indicator Imputation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Data ingestion \n",
    "df= pd.read_csv('titanic_toy.csv')\n",
    "df=df[['Age', 'Fare', 'Survived']]\n",
    "\n",
    "#Train-Test split\n",
    "x=df.iloc[:, :-1]\n",
    "y=df.iloc[:, -1]\n",
    "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "##Before Missing indicator:Imputing with mean value \n",
    "si1=SimpleImputer(strategy='mean')\n",
    "x_train_trf1=si1.fit_transform(x_train)\n",
    "x_test_trf1=si1.transform(x_test)\n",
    "clf1=LogisticRegression()\n",
    "clf1.fit(x_train_trf1, y_train)\n",
    "y_pred_trf1=clf1.predict(x_test_trf1)\n",
    "print(\"Accuracy before imputation\", accuracy_score(y_test, y_pred_trf1))\n",
    "\n",
    "#After Imputation\n",
    "si2=SimpleImputer(add_indicator=True)\n",
    "x_train_trf2=si2.fit_transform(x_train)\n",
    "x_test_trf2=si2.transform(x_test)\n",
    "clf2=LogisticRegression()\n",
    "clf2.fit(x_train_trf2, y_train)\n",
    "y_pred_trf2=clf2.predict(x_test_trf2)\n",
    "print(\"Accuracy score after imputation\", accuracy_score(y_test, y_pred_trf2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:magenta; text-align:left; font-size:140%; font-weight:bold;\">\n",
    "3.B.1.C.ii) GridSearchCV  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "df = df[['pclass', 'sex', 'age', 'parch', 'fare', 'embarked', 'survived']]\n",
    "\n",
    "# Split dataset into features (X) and target (y)\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline for numerical features: Imputation + Scaling\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with mean\n",
    "    ('scaler', StandardScaler())  # Scale numerical values\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features: Imputation + OneHotEncoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing categorical values\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # OneHotEncode categorical features\n",
    "])\n",
    "# Combine both pipelines into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, ['age', 'fare']),  # Apply to numeric features\n",
    "    ('cat', categorical_transformer, ['sex', 'embarked'])  # Apply to categorical features\n",
    "])\n",
    "# Create a final pipeline with preprocessing and model\n",
    "pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "display(pipe)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1, 10],  # Regularization strength\n",
    "    'classifier__solver': ['liblinear', 'lbfgs'],  # Solver for optimization\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the model using GridSearchCV\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy on Test Set: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:deeppink; text-align:left; font-size:170%; font-weight:bold;\">\n",
    "3.B.2) Mutivariate Imputation</h2>\n",
    "<h2 style=\"color:magenta; text-align:left; font-size:140%; font-weight:bold;\">\n",
    "3.B.2.i) KNN imputer  </h2>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.7430167597765364\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Data ingetions \n",
    "df=sns.load_dataset('titanic')\n",
    "df=df[['age','pclass' ,'fare', 'survived' ]]\n",
    "# df.info()\n",
    "#TRAIN-TEST SPLIT \n",
    "x=df.iloc[:, :-1]\n",
    "y=df.iloc[:, -1]\n",
    "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#KNN IMPUTER to impute the missing values \n",
    "        # knn=KNNImputer(missing_values=nan, \n",
    "        #                n_neighbors=5, \n",
    "        #                weights='uniform', \n",
    "        #                metric='nan_euclidean', \n",
    "        #                copy=True, \n",
    "        #                add_indicator=False,\n",
    "        #                keep_empty_features=False)\n",
    "knn=KNNImputer() #taking default parameters \n",
    "x_train_trf=knn.fit_transform(x_train)\n",
    "x_test_trf=knn.transform(x_test)\n",
    "\n",
    "#Model trainng \n",
    "lr=LogisticRegression()\n",
    "lr.fit(x_train_trf, y_train)\n",
    "y_pred=lr.predict(x_test_trf)\n",
    "print(\"accuracy_score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:magenta; text-align:left; font-size:140%; font-weight:bold;\">\n",
    "3.B.2.ii) MICE imputer  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before imputation:\n",
      " pclass        0\n",
      "sex           0\n",
      "age         177\n",
      "parch         0\n",
      "fare          0\n",
      "embarked      2\n",
      "dtype: int64\n",
      "Missing values after imputation:\n",
      " pclass      0\n",
      "sex         0\n",
      "age         0\n",
      "parch       0\n",
      "fare        0\n",
      "embarked    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load Titanic dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "df = df[['pclass', 'sex', 'age', 'parch', 'fare', 'embarked']]  # Reduced features for clarity\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values before imputation:\\n\", df.isnull().sum())\n",
    "\n",
    "# Step 1: Fill missing values with the mean for age\n",
    "age_imputer = SimpleImputer(strategy='mean')\n",
    "df['age'] = age_imputer.fit_transform(df[['age']]).ravel()  # Use ravel() to convert 2D array to 1D\n",
    "\n",
    "# Step 2: Fill missing values with the most frequent value for 'embarked'\n",
    "embarked_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df['embarked'] = embarked_imputer.fit_transform(df[['embarked']]).ravel()  # Use ravel() here as well\n",
    "\n",
    "# The dataframe 'df' is now the Base Table with imputed values\n",
    "print(\"Missing values after imputation:\\n\", df.isnull().sum())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
