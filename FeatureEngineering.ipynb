{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Feature Scalling \n",
    "### 1.1) Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "df=pd.read_csv('Social_Network_Ads.csv')\n",
    "# df.sample(5)\n",
    "df1=df[['Age', 'EstimatedSalary', 'Purchased']]\n",
    "\n",
    "# TRAIN-TEST SPLIT \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test=train_test_split(\n",
    "    df1.drop('Purchased', axis=1), #removes 'Purchased' col from df, leaving only the features (X). \n",
    "    df1['Purchased'],      # selects the 'Purchased' column as the target variable (y)\n",
    "    test_size=0.3,         #means 30% of the data is used for testing, and 70% is used for training.\n",
    "    random_state=0)        #ensures that the split is reproducible\n",
    "print(x_train.shape, x_test.shape)\n",
    "\n",
    "\n",
    "#STANDARD SCALAR \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()    #computes mean & SD for each feature in the training set and then uses those values to scale both the training and test sets.\n",
    "\n",
    "#fit the scaler to the train set, it will learn the parameters\n",
    "scaler.fit(x_train) #it calculate mean&SD in for each feature in x_train\n",
    "\n",
    "#transform train and test sets (Rem: we train from only x_train but transdorm both)\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "## ACCESSMENT OF SCALING \n",
    "\n",
    "\n",
    "#A) Crosschecking whether scalling has done or not (is mean=0 & SD=1 then done)\n",
    "x_train_scaled=pd.DataFrame(x_train_scaled, columns=x_train.columns) #convert array into DF \n",
    "x_test_scaled=pd.DataFrame(x_test_scaled, columns=x_test.columns) \n",
    "round(x_train_scaled.describe(), 2)\n",
    "\n",
    "\n",
    "#B) Now visualising the Distribution \n",
    "fig, (ax1, ax2) =plt.subplots(ncols=2, figsize=(5,2))\n",
    "ax1.scatter(x_train['Age'], x_train['EstimatedSalary'])\n",
    "ax1.set_title('Before Scaling')\n",
    "ax2.scatter(x_train_scaled['Age'], x_train_scaled['EstimatedSalary'], color='r')\n",
    "ax2.set_title('After Scaling')\n",
    "# plt.show()\n",
    "\n",
    "#C) Creating Density plot: in ord\n",
    "fig, (ax1, ax2) =plt.subplots(ncols=2, figsize=(5,2))\n",
    "sns.kdeplot(x_train['Age'], ax=ax1)\n",
    "sns.kdeplot(x_train['EstimatedSalary'], ax=ax1)\n",
    "ax1.set_title('Before Scaling')\n",
    "sns.kdeplot(x_train_scaled['Age'], ax=ax2)\n",
    "sns.kdeplot(x_train_scaled['EstimatedSalary'], ax=ax2)\n",
    "ax2.set_title('After Scaling')\n",
    "\n",
    "#D) Model performance comparison before/after scalling \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr_scaled = LogisticRegression()\n",
    "\n",
    "lr.fit(x_train,y_train)    #trained with unscaled value \n",
    "lr_scaled.fit(x_train_scaled,y_train) # trained with scaled values\n",
    "\n",
    "y_pred = lr.predict(x_test)\n",
    "y_pred_scaled = lr_scaled.predict(x_test_scaled)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Actual\", accuracy_score(y_test, y_pred))\n",
    "print(\"Scaled\", accuracy_score(y_test, y_pred_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "df=pd.read_csv('wine.csv')\n",
    "df.sample(5)\n",
    "df=df[['Wine',\t'Alcohol', \t'Malic.acid']].rename(columns={'Wine':'Wine Class'})\n",
    "\n",
    "# TRAIN-TEST SPLIT \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test=train_test_split(\n",
    "    df.drop('Wine Class', axis=1), #removes 'Wine class' col from df, leaving only the features (X). \n",
    "    df['Wine Class'],      # selects the 'Wine Class' column as the target variable (y)\n",
    "    test_size=0.3,         #means 30% of the data is used for testing, and 70% is used for training.\n",
    "    random_state=0)        #ensures that the split is reproducible\n",
    "print(x_train.shape, x_test.shape)\n",
    "\n",
    "\n",
    "# Normalisarion: MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()    #computes Mean & Max for each feature in the training set and then uses those values to scale both the training and test sets.\n",
    "\n",
    "#fit the scaler to the train set, it will learn the parameters\n",
    "scaler.fit(x_train) #it calculate mean&SD in for each feature in x_train\n",
    "\n",
    "#transform train and test sets (Rem: we train from only x_train but transdorm both)\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "#ACCESSMENT OF SCALING \n",
    "\n",
    "\n",
    "#A) Crosschecking whether scalling has done or not (is mean=0 & SD=1 then done)\n",
    "x_train_scaled=pd.DataFrame(x_train_scaled, columns=x_train.columns) #convert array into DF \n",
    "x_test_scaled=pd.DataFrame(x_test_scaled, columns=x_test.columns) \n",
    "round(x_train_scaled.describe(), 2)\n",
    "\n",
    "\n",
    "#B) Now visualising the Distribution \n",
    "fig, (ax1, ax2) =plt.subplots(ncols=2, figsize=(5,2))\n",
    "ax1.scatter(x_train['Alcohol'], x_train['Malic.acid'],c=y_train )\n",
    "ax1.set_title('Before Scaling')\n",
    "ax2.scatter(x_train_scaled['Alcohol'], x_train_scaled['Malic.acid'], c=y_train )\n",
    "ax2.set_title('After Scaling')\n",
    "# plt.show()\n",
    "\n",
    "#C) Creating Density plot: in ord\n",
    "fig, (ax1, ax2) =plt.subplots(ncols=2, figsize=(5,2))\n",
    "sns.kdeplot(x_train['Alcohol'], ax=ax1)\n",
    "sns.kdeplot(x_train['Malic.acid'], ax=ax1)\n",
    "ax1.set_title('Before Scaling')\n",
    "sns.kdeplot(x_train_scaled['Alcohol'], ax=ax2)\n",
    "sns.kdeplot(x_train_scaled['Malic.acid'], ax=ax2)\n",
    "ax2.set_title('After Scaling')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Catagorical Data Encoding\n",
    "### 2.1) Ordinal Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "df=pd.read_csv(\"Customer Purchase.csv\")\n",
    "# df.sample(5)\n",
    "df=df.iloc[:, 3:]\n",
    "df.sample(5)\n",
    "\n",
    "\n",
    "#TRAIN TEST SPLIT \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test =train_test_split(\n",
    "    df.iloc[:, :2], df.iloc[:, -1], \n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "print(x_train.shape, x_test.shape)\n",
    "\n",
    "\n",
    "#ENCODING INPUT COLUMNS\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "oe=OrdinalEncoder(categories=[['School', 'UG', 'PG'], [ 'Poor','Average', 'Good']]) #telling the order of ordinal data \n",
    "#fit the ordinal encoder to the train set\n",
    "oe.fit(x_train) #fitting meaning assigning the numerical data instead of ordinal \n",
    "\n",
    "#Transform the x_train and x_test \n",
    "x_train_encoded=oe.transform(x_train)\n",
    "x_test_encoded=oe.transform(x_test)\n",
    "# print(x_train_encoded)\n",
    "\n",
    "#ENCODING OUTPUT COLUMNS\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "#Fiting the label encoder \n",
    "le.fit(y_train)\n",
    "#transform\n",
    "y_train_labelencoded=le.transform(y_train)\n",
    "y_test_labelencoded=le.transform(y_test)\n",
    "# print(y_test_labelencoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) One Hot Encoding(OHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load dataset and select relevant columns\n",
    "df = pd.read_csv('car_details.csv')\n",
    "df = df[['name', 'km_driven', 'fuel', 'owner', 'selling_price']]\n",
    "df.sample(5)\n",
    "\n",
    "# TRAIN TEST SPLIT \n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df.iloc[:, 0:4], df.iloc[:, -1], test_size=0.2, random_state=0)\n",
    "\n",
    "#ONEHOTENCODER\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # use 'sparse_output' instead of 'sparse'\n",
    "\n",
    "# Fit the encoder on the categorical columns of training data\n",
    "x_train_categorical = ohe.fit_transform(x_train[['fuel', 'owner']])\n",
    "\n",
    "# Transform the test data's categorical columns\n",
    "x_test_categorical = ohe.transform(x_test[['fuel', 'owner']])\n",
    "\n",
    "\n",
    "\n",
    "# # Convert back to DataFrame for easier handling\n",
    "x_train_categorical = pd.DataFrame(x_train_categorical, columns=ohe.get_feature_names_out(['fuel', 'owner']))\n",
    "x_train_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Simultaneous Transformation \n",
    "## 4.1) Column transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset and select relevant columns\n",
    "df=pd.read_csv(\"dummy_covid_data.csv\")\n",
    "df.sample(5)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], test_size=0.2, random_state=0)\n",
    "\n",
    "#COLUMN TRANSFORMER\n",
    "transformer= ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('tnf1', OrdinalEncoder(categories=[['mild', 'strong']]), ['cough']),  # Apply StandardScaler to numerical columns\n",
    "        ('fnf2', OneHotEncoder(sparse_output=False, drop='first'),['gender', 'city'])  # Apply OneHotEncoder to categorical columns\n",
    "    ]\n",
    "    , remainder='passthrough'\n",
    ")\n",
    "# Fit the transformer into the training dataset\n",
    "x_train_transform = transformer.fit_transform(x_train)\n",
    "\n",
    "# transform the test dataset \n",
    "x_test_transform=transformer.transform(x_test)\n",
    "print(x_train_transform.shape, x_test_transform.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "#1) DATA Ingestion\n",
    "df=sns.load_dataset('titanic')\n",
    "df=df.iloc[:, 0:8]\n",
    "df.info()\n",
    "df.isnull().sum()\n",
    "\n",
    "# #Split Train-Test data split \n",
    "x_train, x_test, y_train, y_test=train_test_split(\n",
    "    df.iloc[:, 1:8],df.iloc[:, 0], test_size=0.2, random_state=0)\n",
    "x_train\n",
    "#2) Creating PIPELINE FOR TRANSFORMATION \n",
    "#Transformation: 1st define the Individual transformation then execute in sequence through pipeline  \n",
    "#     1st: Imputation transformation in age & embark ,\n",
    "#     2nd: OneHotEncoding in Sex and embark \n",
    "#     3rd: scalling all the columns \n",
    "#     4th: Feature selection \n",
    "#     5th: Train the model \n",
    "\n",
    "#2.1) IMPUTATION TRANSFORMATION \n",
    "trf1=ColumnTransformer([\n",
    "    ('impute-age', SimpleImputer(), [2]), \n",
    "    ('impute-embark', SimpleImputer(strategy='most_frequent'), [6])\n",
    "], remainder='passthrough')\n",
    "\n",
    "#2.2) ONE HOT ENCODING \n",
    "trf2=ColumnTransformer([\n",
    "    ('ohe-sex-embark', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), [1, 6])\n",
    "], remainder='passthrough')\n",
    "\n",
    "#2.3) Scaling \n",
    "trf3=ColumnTransformer([\n",
    "    ('age-scaling', MinMaxScaler(), slice(0,10)) #scaling all cols after trf1&2\n",
    "]) \n",
    "\n",
    "#2.4) Feature Selections\n",
    "trf4=SelectKBest(score_func=chi2, k=8) # Selectinf 8/10 best cols \n",
    "\n",
    "#2.5) model \n",
    "trf5=DecisionTreeClassifier()\n",
    "\n",
    "#3)CREATE PIPELINE \n",
    "pipe=Pipeline([\n",
    "    ('trf1', trf1),\n",
    "    ('trf2', trf2),\n",
    "    ('trf3', trf3),\n",
    "    ('trf4', trf4),\n",
    "    ('trf5', trf5),\n",
    "])\n",
    "\n",
    "#4) TRAIN the model\n",
    "pipe.fit(x_train, y_train)\n",
    "\n",
    "#5) Predict: since model is trained we predict the value \n",
    "y_pred= pipe.predict(x_test)\n",
    "y_pred\n",
    "\n",
    "#6) CALCULATE ACCURACY \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "#7) CROSS VALIDATION using pipeline \n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(pipe, x_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "\n",
    "#8)HYPERTUNING Using GridsearchCV \n",
    "params={\n",
    "    'trf5__max_depth':[1,2,3,4,5, None]\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid=GridSearchCV(pipe, params, cv=5, scoring='accuracy')\n",
    "grid.fit(x_train, y_train)\n",
    "print(\"Best score=\",grid.best_score_, \"Max_depth=\", grid.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "#9)Exporting the PIPELINE\n",
    "import pickle\n",
    "pickle.dump(pipe, open('pipe.pkl', 'wb'))\n",
    "\n",
    "\n",
    "#10)CODE IN PRODUCTION \n",
    "import pickle \n",
    "import numpy as np\n",
    "pipe=pickle.load(open('pipe.pkl', 'rb'))\n",
    "#Assume user input \n",
    "test_input1 = np.array([2, 'male', 31.0, 0, 0, 10.5, 'S'], dtype='object').reshape(1,7)\n",
    "pipe.predict(test_input1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Mathematical Transforms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QQ Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate some data (e.g., normally distributed data)\n",
    "df = sns.load_dataset('titanic')#stats.norm.rvs(size=1000)\n",
    "# Create QQ plot\n",
    "stats.probplot(df['age'], dist=\"norm\", plot=plt)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1) Functional Trasformers \n",
    "#### 5.1.a) Log Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "#dataset\n",
    "data = sns.load_dataset('titanic')\n",
    "# Apply log transformation\n",
    "log_transformed_data = np.log(data['age'])\n",
    "#ploting data before and after log transformer \n",
    "fig, (ax1, ax2) =plt.subplots(ncols=2, figsize=(5,2))\n",
    "sns.histplot(data['age'], ax=ax1,  kde=True)\n",
    "ax1.set_title('Before Transformation')\n",
    "sns.histplot(log_transformed_data, ax=ax2,  kde=True)\n",
    "ax2.set_title('After Log Transformation')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.b) Reciprocal  Transformermer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "# Example data\n",
    "data = sns.load_dataset('titanic')\n",
    "# Apply log transformation\n",
    "reciprocal_transformed_data = 1/(data['age'].dropna())\n",
    "#ploting data before and after log transformer \n",
    "fig, (ax1, ax2) =plt.subplots(ncols=2, figsize=(5,2))\n",
    "sns.histplot(data['age'].dropna(), ax=ax1, kde=True)\n",
    "ax1.set_title('Before Transformation')\n",
    "sns.histplot(log_transformed_data.dropna(),ax=ax2, kde=True)\n",
    "ax2.set_title('After Reciprocal Transformation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.c) x-Square transform  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "# Example data\n",
    "data = sns.load_dataset('titanic')\n",
    "# Apply x-square transformation\n",
    "square_transformed_data = (data['age'].dropna())**2\n",
    "#ploting data before and after log transformer \n",
    "fig, (ax1, ax2) =plt.subplots(ncols=2, figsize=(5,2))\n",
    "sns.histplot(data['age'].dropna(), ax=ax1, kde=True)\n",
    "ax1.set_title('Before Transformation')\n",
    "sns.histplot(square_transformed_data.dropna(),ax=ax2, kde=True)\n",
    "ax2.set_title('After X-square Transformation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.d) Square root transform  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "# Example data\n",
    "data = sns.load_dataset('titanic')\n",
    "# Apply square root transformation\n",
    "square_root_transformed_data = np.sqrt(data['age'].dropna())\n",
    "#ploting data before and after square root transformer \n",
    "fig, (ax1, ax2) =plt.subplots(ncols=2, figsize=(5,2))\n",
    "sns.histplot(data['age'].dropna(), ax=ax1, kde=True)\n",
    "ax1.set_title('Before Transformation')\n",
    "sns.histplot(square_root_transformed_data.dropna(),ax=ax2, kde=True)\n",
    "ax2.set_title('After square-root Transformation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.e) Custom transform  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load example data\n",
    "data = sns.load_dataset('titanic')\n",
    "\n",
    "# Custom transformation function\n",
    "def custom_transform(x, threshold=30):\n",
    "    if x < threshold:\n",
    "        return np.sqrt(x)\n",
    "    else:\n",
    "        return x ** 2\n",
    "\n",
    "# Apply custom transformation to the 'age' column (handle missing values)\n",
    "custom_transformed_data = data['age'].dropna().apply(custom_transform)\n",
    "\n",
    "# Plotting data before and after custom transformation\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "\n",
    "# Original data (before transformation)\n",
    "sns.histplot(data['age'].dropna(), ax=ax1, kde=True)\n",
    "ax1.set_title('Original Age Data')\n",
    "\n",
    "# Custom transformed data (after transformation)\n",
    "sns.histplot(custom_transformed_data, ax=ax2, kde=True, color='purple')\n",
    "ax2.set_title('Custom Transformed Age Data')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2) Functional Transform  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# 1) LOAD DATASET \n",
    "df=sns.load_dataset('titanic')\n",
    "df=df[['age', 'fare', 'survived']]\n",
    "df.info()\n",
    "\n",
    "#2.1) Imputation of Age \n",
    "df['age'].fillna(df['age'].mean(), inplace=True)\n",
    "df.isnull().sum() #check for Null values\n",
    "\n",
    "#2.2) Extract the X and y from df  & DO Train-test split  \n",
    "x=df.iloc[:, 0:2]\n",
    "y=df.iloc[:, -1]\n",
    "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#2.3) Checking the Normality of the data then Transforming \n",
    "#Ploting age \n",
    "plt.figure(figsize=(9,2))\n",
    "plt.subplot(121)\n",
    "sns.histplot(x_train['age'], kde=True) \n",
    "plt.subplot(122)\n",
    "stats.probplot(x_train['age'], dist='norm', plot=plt)\n",
    "plt.show()\n",
    "#ploting fare \n",
    "plt.figure(figsize=(9,2))\n",
    "plt.subplot(121)\n",
    "sns.histplot(x_train['fare'], kde=True) \n",
    "plt.subplot(122)\n",
    "stats.probplot(x_train['fare'], dist='norm', plot=plt)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#3) BEFORE NORMALIZATION: Fit in the model and check the accuracy \n",
    "clf1=LogisticRegression()\n",
    "clf2=DecisionTreeClassifier()\n",
    "#traing the model \n",
    "clf1.fit(x_train, y_train)\n",
    "clf2.fit(x_train, y_train)\n",
    "#prediction \n",
    "y_pred1=clf1.predict(x_test)\n",
    "y_pred2=clf2.predict(x_test)\n",
    "#calculating the accuracy \n",
    "print('LR Accuracy=', accuracy_score(y_test, y_pred1))\n",
    "print('DT Accuracy=', accuracy_score(y_test, y_pred2))\n",
    "\n",
    "#4) AFTER NORMALIZATION: Fit in the model and check the accuracy \n",
    "trf=FunctionTransformer(func=np.log1p)\n",
    "\n",
    "x_train_transform=trf.fit_transform(x_train)\n",
    "x_test_transform=trf.transform(x_test)\n",
    "\n",
    "clf1=LogisticRegression()\n",
    "clf2=DecisionTreeClassifier()\n",
    "#traing the model \n",
    "clf1.fit(x_train_transform, y_train)\n",
    "clf2.fit(x_train_transform, y_train)\n",
    "#prediction \n",
    "y_pred_tranformed1=clf1.predict(x_test_transform)\n",
    "y_pred_tranformed2=clf2.predict(x_test_transform)\n",
    "#calculating the accuracy \n",
    "print('LR Accuracy=', accuracy_score(y_test, y_pred_tranformed1))\n",
    "print('DT Accuracy=', accuracy_score(y_test, y_pred_tranformed2))\n",
    "\n",
    "#5)CROSS VALIDATING THE RESULT FOR CONFIRMATION \n",
    "x_transformed=trf.fit_transform(x)\n",
    "clf1=LogisticRegression()\n",
    "clf2=DecisionTreeClassifier()\n",
    "print('LR',np.mean(cross_val_score(clf1, x_transformed, y, scoring='accuracy', cv=15)))\n",
    "print('DT',np.mean(cross_val_score(clf2, x_transformed, y, scoring='accuracy', cv=10)))\n",
    "\n",
    "\n",
    "#6) NOT CHecking QQ plot of BEFORE AND AFTER FUNCTIONTRANSFORM\n",
    "#before transform\n",
    "plt.figure(figsize=(9,2))\n",
    "plt.subplot(121)\n",
    "stats.probplot(x_train['fare'], dist='norm', plot=plt)\n",
    "#after Transform  \n",
    "plt.subplot(122)\n",
    "stats.probplot(x_train_transform['fare'], dist='norm', plot=plt)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3) Power Transform  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "#Data Ingention \n",
    "df=pd.read_csv('concrete.csv')\n",
    "df.sample(2)\n",
    "df.info()\n",
    "# TRAIN-TEST SPLIT\n",
    "x=df.iloc[:, :-1]\n",
    "y=df.iloc[:, -1] \n",
    "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#BEFORE Transdormation: Trainig the model, predict, Accuracy \n",
    "lr=LinearRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "y_pred=lr.predict(x_test)\n",
    "print('LR accuracy=', r2_score(y_test, y_pred))\n",
    "print('lr_crossvalidation', np.mean(cross_val_score(lr, x, y, scoring='r2', cv=10)))\n",
    "\n",
    "#Plotting the QQ plot \n",
    "for i in x_train.columns:\n",
    "    plt.figure(figsize=(6,2))\n",
    "    plt.subplot(121)\n",
    "    sns.histplot(x_train[i], kde=True)\n",
    "    plt.title(i)\n",
    "    plt.subplot(122)\n",
    "    stats.probplot(x_train[i], dist='norm', plot=plt)\n",
    "    plt.title(i)\n",
    "    plt.show()\n",
    "\n",
    "#TRANSFORMING \n",
    "pt=PowerTransformer(method='box-cox', standardize=True)\n",
    "x_train_transform= pt.fit_transform(x_train+0.000001)\n",
    "x_test_transform=pt.transform(x_test+0.000001)\n",
    "\n",
    "lr=LinearRegression()\n",
    "lr.fit(x_train_transform, y_train)\n",
    "y_pred_tranform=lr.predict(x_test_transform)\n",
    "\n",
    "print('LR accuracy after trans=', r2_score(y_test, y_pred_tranform))\n",
    "#cross validation after transform\n",
    "pt=PowerTransformer(method='box-cox', standardize=True)\n",
    "x_trans=pt.fit_transform(x+0.000001)\n",
    "print('LR_Trans_cross valid=', np.mean(cross_val_score(lr, x_trans, y, scoring='r2', cv=10)))\n",
    "\n",
    "#Ploting the graph Before and After Transformation \n",
    "x_trans = pd.DataFrame(x_trans, columns=x_train.columns)\n",
    "for i in x_trans.columns:\n",
    "    plt.figure(figsize=(6, 2))\n",
    "    plt.subplot(121)\n",
    "    sns.histplot(x_train[i], kde=True)\n",
    "    plt.title(f\"Original: {i}\")\n",
    "    plt.subplot(122)\n",
    "    column_index = x_train.columns.get_loc(i)  # Get the index of the column\n",
    "    sns.histplot(x_train_transform[:, column_index], kde=True)  # Use index\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Numerical Encoding \n",
    "## 6.1) Binning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load the dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "df = df[['age', 'fare', 'survived']]\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df[['age', 'fare']]  # Features (age and fare)\n",
    "y = df['survived']       # Target (survived)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function for discretization\n",
    "def discretize(bins, strategy):\n",
    "    kbin_age = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy=strategy)\n",
    "    kbin_fare = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy=strategy)\n",
    "    \n",
    "    trf = ColumnTransformer([\n",
    "        ('age', kbin_age, [0]),\n",
    "        ('fare', kbin_fare, [1])\n",
    "    ])\n",
    "    \n",
    "    X_trf = trf.fit_transform(X_train)\n",
    "    \n",
    "    # Model training and cross-validation\n",
    "    model = DecisionTreeClassifier()\n",
    "    cv_score = np.mean(cross_val_score(model, X_trf, y_train, cv=10, scoring='accuracy'))\n",
    "    print(f\"Cross-Validation Accuracy: {cv_score}\")\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(6, 2))\n",
    "    plt.subplot(121)\n",
    "    plt.hist(X_train['age'], bins=10, color='blue', alpha=0.7)\n",
    "    plt.title(\"Age Before\")\n",
    "    plt.subplot(122)\n",
    "    plt.hist(X_trf[:, 0], bins=10, color='red', alpha=0.7)\n",
    "    plt.title(\"Age After\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(6, 2))\n",
    "    plt.subplot(121)\n",
    "    plt.hist(X_train['fare'], bins=10, color='blue', alpha=0.7)\n",
    "    plt.title(\"Fare Before\")\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.hist(X_trf[:, 1], bins=10, color='red', alpha=0.7)\n",
    "    plt.title(\"Fare After\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "discretize(5, 'kmeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2) Binnerization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load and prepare the dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "df=df[['age', 'fare', 'sibsp', 'parch', 'survived']]\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Create a new feature 'family' by combining 'SibSp' and 'Parch'\n",
    "df['family'] = df['sibsp'] + df['parch']\n",
    "df.drop(columns=['sibsp', 'parch'], inplace=True)\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = df.drop(columns=['survived'])\n",
    "y = df['survived']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Apply binarization to the 'family' feature\n",
    "trf = ColumnTransformer([\n",
    "    ('bin', Binarizer(copy=False), ['family'])\n",
    "], remainder='passthrough')\n",
    "\n",
    "X_train_trf = trf.fit_transform(X_train)\n",
    "X_test_trf = trf.transform(X_test)\n",
    "\n",
    "# Display the transformed training data\n",
    "pd.DataFrame(X_train_trf, columns=['family', 'age', 'fare']).head()\n",
    "\n",
    "# Train a Decision Tree classifier on the binarized data\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train_trf, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred2 = clf.predict(X_test_trf)\n",
    "print(\"Accuracy with binarization:\", accuracy_score(y_test, y_pred2))\n",
    "\n",
    "# Cross-validation with binarized data\n",
    "X_trf = trf.fit_transform(X)\n",
    "print(\"Cross-validation score with binarization:\", np.mean(cross_val_score(DecisionTreeClassifier(), X_trf, y, cv=10, scoring='accuracy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) MISSING VALUES- Handlings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1) Initial insight \n",
    "#### 7.1.a) Glance of Mising values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "df=pd.read_csv('heart_disease.csv')\n",
    "df.sample(3)\n",
    "miss_value=df.isnull().sum()\n",
    "miss_value[miss_value>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.b) Proportion of missing values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('heart_disease.csv')\n",
    "def missing_values(df, output='summary'):\n",
    "    #1) Total missing values per column\n",
    "    miss_val = df.isnull().sum()\n",
    "    #2) Percentage of missing values per column\n",
    "    percentage_miss_val = (miss_val / len(df)) * 100\n",
    "    #3) Create a DataFrame with the results\n",
    "    miss_val_table = pd.concat([miss_val, percentage_miss_val], axis=1)\n",
    "    miss_val_table.columns= ['Missing Values','% of Total Values']\n",
    "    # 5) Sort the table by percentage of missing values in descending order\n",
    "    sorted_miss_val_table = miss_val_table[miss_val_table.iloc[:, 0] != 0].sort_values('% of Total Values', ascending=False).round(1)\n",
    "    # retun based on the specific value \n",
    "    if output=='miss_value':\n",
    "        return miss_value\n",
    "    elif output=='percentage_miss_val':\n",
    "        return percentage_miss_val\n",
    "    else :\n",
    "        return sorted_miss_val_table\n",
    "# Example usage with your data\n",
    "missing_data_summary = missing_values(data, output='miss_value')\n",
    "miss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2) Misingno libray "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import missingno as msno\n",
    "df=pd.read_csv('heart_disease.csv')\n",
    "#1) Bar chart \n",
    "msno.bar(df)\n",
    "#2) Matrix chart\n",
    "msno.matrix(df)\n",
    "#3) Heat map \n",
    "msno.heatmap(df)\n",
    "#4) dendragram  \n",
    "msno.dendrogram(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Methods for handling missing values  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1) Deletion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "df=pd.read_csv('data_science_job.csv')\n",
    "#Infomation about missing values\n",
    "miss_value=df.isnull().sum()\n",
    "per_miss_values=100*miss_value/len(df)\n",
    "miss_val_info_table= pd.concat([miss_value, per_miss_values], axis=1).round(1)\n",
    "miss_val_info_table.columns=['Missing Values','% of Total Values'] \n",
    "miss_val_info_table[miss_val_info_table.iloc[:, 0] !=0]\n",
    "\n",
    "\n",
    "#1) Deletions \n",
    "#------ i want to delete those rows in which % of missing value is <5% \n",
    "cols_no_miss= []\n",
    "for var in df.columns:\n",
    "    percent_missing= df[var].isnull().sum()*100/len(df)\n",
    "    if 0<percent_missing<5:\n",
    "        cols_no_miss.append(var)\n",
    "type(cols_no_miss)\n",
    "#-------------df with cols having <5% missing data\n",
    "df_new=df[cols_no_miss]\n",
    "#-------------dropping the missing value in this cols \n",
    "df_new=df_new.dropna()\n",
    "print(f\"Original shape: {df.shape}, shape after deletions: {df_new.shape}\")\n",
    "\n",
    "#---------------Now checking whetehr Data is MCAR:------------\n",
    "\n",
    "#<<<<<<<---------FOR NUMERICAL VARIABLES----------->>>>>>\n",
    "\n",
    "# A) Ploting the Histogram \n",
    "import matplotlib.pyplot as plt \n",
    "fig=plt.figure(figsize=(8,3))\n",
    "plt.subplot(121)\n",
    "df['training_hours'].hist(bins=50,density=True, color='r' )\n",
    "df_new['training_hours'].hist(bins=50, density=True ,color='g' )\n",
    "#B) Probability graph\n",
    "plt.subplot(122) \n",
    "df['training_hours'].plot.density(color='r' )\n",
    "df_new['training_hours'].plot.density(color='g')\n",
    "plt.show()\n",
    "\n",
    "#<<<<<<<---------FOR CATEGORICAL VARIABLES----------->>>>>>\n",
    "#RATIO of observation per categories  \n",
    "temp=pd.concat([\n",
    "    df['enrolled_university'].value_counts()/len(df), #in original data \n",
    "    df_new['enrolled_university'].value_counts()/len(df)  #in new df dat\n",
    "    \n",
    "], axis=1)\n",
    "temp.columns=['original', 'after deletion']\n",
    "print(temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2) Imputation \n",
    "\n",
    "### 8.2.a) Univariate Imputation: (Mean, median, Mode)\n",
    "##### >> USINNG PANDAS LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "df=pd.read_csv('titanic_toy.csv')\n",
    "#checing the info % of missiness \n",
    "df.isnull().mean()\n",
    "\n",
    "#TRAIN-TEST SPLIT \n",
    "x=df.iloc[:, :-1]\n",
    "y=df.iloc[:, -1]\n",
    "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.2 , random_state=0)\n",
    "\n",
    "#Transformation -----USING PANDAS----------- as age and fare have null values\n",
    "mean_age=x_train['Age'].mean()\n",
    "median_age=x_train['Age'].median()\n",
    "mean_fare=x_train['Fare'].mean()\n",
    "median_fare=x_train['Fare'].median()\n",
    "# creating X_train and test DF having 4-Imputed cols\n",
    "x_train['age_mean']=x_train['Age'].fillna(mean_age)\n",
    "x_train['age_median']=x_train['Age'].fillna(median_age)\n",
    "x_train['fare_mean']=x_train['Fare'].fillna(mean_fare)\n",
    "x_train['fare_median']=x_train['Fare'].fillna(median_fare)\n",
    "\n",
    "\n",
    "#Checking following after imputations \n",
    "#a) Change in shapep--check change in variance\n",
    "print(\"Original Age var:\", x_train['Age'].var(),\"\\n\"\n",
    "      \"      After Mean-Age Imputation var:\",x_train['age_mean'].var(),\"\\n\"\n",
    "      \"      After Median-Age Imputation var:\",x_train['age_median'].var()\n",
    "     )\n",
    "print(\"Original Fare var:\", x_train['Fare'].var(),\"\\n\"\n",
    "      \"      After Mean-Fare Imputation var:\", x_train['fare_mean'].var(),\"\\n\"\n",
    "      \"      After Median-Fare Imputation var:\", x_train['fare_median'].var()\n",
    "     )\n",
    "#b)plotting to check the distribution \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "x_train['Age'].plot(kind='kde', color='r' )\n",
    "x_train['age_mean'].plot(kind='kde', color='b' )\n",
    "x_train['age_median'].plot(kind='kde', color='g' )\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "x_train['Fare'].plot(kind='kde', color='r' )\n",
    "x_train['fare_mean'].plot(kind='kde', color='b' )\n",
    "x_train['fare_median'].plot(kind='kde', color='g' )\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#c) Covariance \n",
    "x_train.cov()\n",
    "\n",
    "#d) Draw box plot \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "x_train[['Age', 'age_median', 'age_median']].boxplot()\n",
    "plt.subplot(122)\n",
    "x_train[['Fare', 'fare_mean', 'fare_median']].boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >> USING Scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#Data ingestion \n",
    "df=pd.read_csv('titanic_toy.csv')\n",
    "\n",
    "#train-test split \n",
    "x=df.iloc[:, :-1]\n",
    "y=df.iloc[:, -1]\n",
    "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Using simple imputer\n",
    "imputer1=SimpleImputer(strategy='mean')\n",
    "imputer2=SimpleImputer(strategy='median')\n",
    "\n",
    "trf=ColumnTransformer([\n",
    "    ('imputer1',imputer1, ['Age']),\n",
    "    ('imputer2', imputer2, ['Fare'])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "#Fit and transform \n",
    "trf.fit(x_train)\n",
    "x_train_trans=trf.transform(x_train)\n",
    "x_test_trans=trf.transform(x_test)\n",
    "\n",
    "#Now Perform the basic check there is np RED FLAG\n",
    "x_train_trans_df=pd.DataFrame(x_train_trans, columns=x_train.columns)\n",
    "\n",
    "#a) Change in shapep--check change in variance\n",
    "print(\"Original Age var:\", x_train['Age'].var(),\"\\n\"\n",
    "      \"After Mean-Age Imputation var:\",x_train_trans_df['Age'].var()\n",
    "     )\n",
    "print(\"Original Fare var:\", x_train['Fare'].var(),\"\\n\"\n",
    "      \"After Median-Fare Imputation var:\", x_train_trans_df['Fare'].var()\n",
    "      )\n",
    "#b)plotting to check the distribution \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "x_train['Age'].plot(kind='kde', color='r')\n",
    "x_train_trans_df['Age'].plot(kind='kde', color='g' )\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "x_train['Fare'].plot(kind='kde', color='r' )\n",
    "x_train_trans_df['Fare'].plot(kind='kde', color='g' )\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#c) Draw box plot \n",
    "df_box = pd.DataFrame({\n",
    "    'Original Age': x_train['Age'],\n",
    "    'Transformed Age': x_train_trans_df['Age'], \n",
    "    'Original Fare': x_train['Fare'],\n",
    "    'Transformed Fare': x_train_trans_df['Fare']\n",
    "    })\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "df_box[['Original Age', 'Transformed Age']].boxplot()\n",
    "plt.subplot(122)\n",
    "df_box[['Original Fare','Transformed Fare']].boxplot()\n",
    "plt.show()\n",
    "#c) Covariance \n",
    "df_box.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.B) Constants imputations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          City\n",
      "0     New York\n",
      "1  Los Angeles\n",
      "2      Unknown\n",
      "3      Chicago\n",
      "4      Unknown\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'City': ['New York', 'Los Angeles', np.NaN, 'Chicago', np.NaN]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Instantiate the SimpleImputer with strategy 'constant' and fill_value 'Unknown'\n",
    "imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "\n",
    "# Fit and transform the data\n",
    "df['City'] = imputer.fit_transform(df[['City']]).ravel()\n",
    "\n",
    "print(df)\n",
    "# df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          City income\n",
      "0     New York    100\n",
      "1  Los Angeles     99\n",
      "2      Unknown  -9999\n",
      "3      Chicago     80\n",
      "4      Unknown  -9999\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'City': ['New York', 'Los Angeles', np.nan, 'Chicago', np.nan], \n",
    "        'income': [100, '99', np.nan, '80', np.nan]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Instantiate the SimpleImputer with strategy 'constant' and fill_value 'Unknown'\n",
    "imputer1 = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "imputer2 = SimpleImputer(strategy='constant', fill_value=-9999)\n",
    "\n",
    "# Fit and transform the data\n",
    "df['City'] = imputer1.fit_transform(df[['City']]).ravel()\n",
    "df['income'] = imputer2.fit_transform(df[['income']]).ravel()\n",
    "print(df)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
